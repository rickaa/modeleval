{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "from sklearn.metrics import *\n",
    "\n",
    "class BaseEvaluator(object):\n",
    "    \"\"\"\n",
    "    A base evaluator object, generate model evaluation result on test data.\n",
    "    Accepted models are base model in sklearn, xgboost, lightgbm and catboost.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def ensure_path(self, file_path):\n",
    "        \"\"\"Helper function to create/ensure the path\"\"\"\n",
    "        directory = os.path.dirname(file_path)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "\n",
    "class RegressionEvaluator(BaseEvaluator):\n",
    "    \"\"\"\n",
    "    For regression problem, generate common evaluation metrics\n",
    "    and plots which are useful to evaluate your model performance\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(RegressionEvaluator, self).__init__()\n",
    "    \n",
    "    \n",
    "    def get_rmsle(self, predicted, real):\n",
    "        \"\"\"Calculate Root Mean Squared Logarithmic Error (RMSLE)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        predicted : \n",
    "\n",
    "        real : ndarray\n",
    "            The real response.\n",
    "\n",
    "        eval_y : ndarray\n",
    "            The predicted response.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        rmsle : float \n",
    "        \"\"\"\n",
    "        sum=0.0\n",
    "        for x in range(len(predicted)):\n",
    "            p = np.log(predicted[x]+1)\n",
    "            r = np.log(real[x]+1)\n",
    "            sum = sum + (p - r)**2\n",
    "        return (sum/len(predicted))**0.5\n",
    "    \n",
    "    \n",
    "    def get_metrics(self, model, eval_X, eval_y):\n",
    "        \"\"\"Calculate common metrics for regression problem.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        model : sklearn/lightgbm/xgboost/catboost regression model object\n",
    "            The model for evaluation.\n",
    "\n",
    "        eval_X : ndarray or pd.DataFrame\n",
    "            The test data's features.\n",
    "\n",
    "        eval_y : ndarray\n",
    "            The test data's response.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.eval_y = eval_y\n",
    "        if isinstance(eval_X, pd.DataFrame):\n",
    "            eval_X = eval_X.values\n",
    "        \n",
    "        self.eval_X = eval_X\n",
    "        self.y_pred = model.predict(self.eval_X)\n",
    "        self.res = self.y_pred - self.eval_y\n",
    "        # metrics\n",
    "        self.mse = mean_squared_error(eval_y, self.y_pred)\n",
    "        self.mae = mean_absolute_error(eval_y, self.y_pred)\n",
    "        self.rmse = np.sqrt(self.mse)\n",
    "        self.rmsle = self.get_rmsle(self.y_pred, eval_y)\n",
    "        self.r2 = r2_score(eval_y, self.y_pred)\n",
    "    \n",
    "    \n",
    "    def res_fit_plot(self):\n",
    "        \"\"\"Residual vs Fitted value plot\"\"\"\n",
    "        plt.scatter(self.y_pred, self.res, c='red')\n",
    "        plt.title(\"Residuals vs Fitted Values plot\")\n",
    "        plt.xlabel(\"Fitted Values\")\n",
    "        plt.ylabel(\"Residuals\")\n",
    "        return plt\n",
    "    \n",
    "    def evaluate(self, model, eval_X, eval_y, plot=True, save=False, save_folder=\"result\"):\n",
    "        \"\"\"Make prediction and generate evaluation report.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : sklearn/lightgbm/xgboost/catboost regression model object\n",
    "            The model for evaluation.\n",
    "\n",
    "        eval_X : ndarray or pd.DataFrame\n",
    "            The test data's features.\n",
    "\n",
    "        eval_y : ndarray\n",
    "            The test data's response.\n",
    "\n",
    "        plot : bool (default=True)\n",
    "            If \"Flase\", return only common metrics. If \"True\", return the \n",
    "            residual vs fitted values plot as well.\n",
    "\n",
    "        save : bool (default=False)\n",
    "            Save the result to path if True.\n",
    "\n",
    "        save_folder : string (default=\"result\")\n",
    "            The folder path to save the result, default is the result folder in the current directory.\n",
    "        \"\"\"\n",
    "        self.get_metrics(model, eval_X, eval_y)\n",
    "        print(\"Evaluation Result\")\n",
    "        print(\"---Common Metrics---\")\n",
    "        print(\"The mse is %0.4f\" % self.mse)\n",
    "        print(\"The mae for 1 is %0.4f\" % self.mae)\n",
    "        print(\"The rmse for 1 is %0.4f\" % self.rmse)\n",
    "        print(\"The rmsle for 0 is %0.4f\" % self.rmsle)\n",
    "        print(\"The r-square for 0 is %0.4f\" % self.r2)\n",
    "        if plot:\n",
    "            fig = self.res_fit_plot()\n",
    "        if save:\n",
    "            super(RegressionEvaluator, self).ensure_path(save_folder)\n",
    "            plot_path = save_folder + 'residual_vs_fittedval.png'\n",
    "            if fig:\n",
    "                fig.savefig(\n",
    "                    plot_path,\n",
    "                    bbox_inches='tight'\n",
    "                )\n",
    "            #Write result into a txt\n",
    "            output = '\\n'.join([\n",
    "                '--Model Evaluation--',\n",
    "                '\\tmse: {mse:.4f}',\n",
    "                '\\tmae: {mae:.4f}',\n",
    "                '\\trmse: {rmse:.4f}',\n",
    "                '\\trmsle: {rmsle:.4f}',\n",
    "                '\\tr-square: {r2:.4f}',\n",
    "                '\\n'\n",
    "            ]).format(\n",
    "                mse = self.mse,\n",
    "                mae = self.mae,\n",
    "                rmse = self.rmse,\n",
    "                rmsle = self.rmsle,\n",
    "                r2 = self.r2\n",
    "            )\n",
    "            result_path = save_folder + 'reg_output.txt'\n",
    "            with open(result_path, 'w+') as f:\n",
    "                f.write(output)\n",
    "            \n",
    "    \n",
    "    def find_best_model(self, models, eval_X=None, eval_y=None, objective=\"mse\"):\n",
    "        \"\"\"Find the best model with the specified objective metric.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        models : list \n",
    "            List of sklearn/lightgbm/xgboost/catboost regression model.\n",
    "            \n",
    "        eval_X : ndarray or pd.DataFrame\n",
    "            The test data's features.\n",
    "\n",
    "        eval_y : ndarray\n",
    "            The test data's response.\n",
    "        \n",
    "        objective: string (default=\"mse\")\n",
    "            The objective metric.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        A single model object.       \n",
    "        \"\"\"\n",
    "        result = np.array([])\n",
    "        for model in models:\n",
    "            y_pred = model.predict(eval_X)\n",
    "            if objective == \"mse\":\n",
    "                result = np.append(result, mean_squared_error(eval_y, y_pred))\n",
    "            if objective == \"mae\":\n",
    "                result = np.append(result, mean_absolute_error(eval_y, y_pred))\n",
    "            if objective == \"rmse\":\n",
    "                result = np.append(result, np.sqrt(mean_squared_error(eval_y, y_pred)))\n",
    "            if objective == \"rmsle\":\n",
    "                result = np.append(result, self.rmsle(y_pred, eval_y))\n",
    "            if objective == \"r2\":\n",
    "                result = np.append(result, r2_score(eval_y, y_pred))\n",
    "        if objective != \"r2\":\n",
    "            print(\"The model with maximum r-square (%s) is the %s th model\" % (result[result.argmin()], result.argmin()+1))\n",
    "            return models[result.argmax()]\n",
    "        else:\n",
    "            print(\"The model with minimum %s (%s) is the %s th model\" % (objective, result[result.argmin()], result.argmin()+1))\n",
    "            return models[result.argmin()]\n",
    "\n",
    "class BinaryEvaluator(BaseEvaluator):\n",
    "    \"\"\"\n",
    "    For binary classification problem, generate common evaluation metrics\n",
    "    and plots which are useful to evaluate your model performance and \n",
    "    determine the threshold.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BinaryEvaluator, self).__init__()\n",
    "        \n",
    "        \n",
    "    def get_metrics(self, model, eval_X, eval_y, threshold=0.5):\n",
    "        \"\"\"Calculate common metrics for binary classification problem\"\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        model : sklearn/lightgbm/xgboost/catboost classification model object\n",
    "            The model for evaluation.\n",
    "\n",
    "        eval_X : ndarray or pd.DataFrame\n",
    "            The test data's features.\n",
    "\n",
    "        eval_y : ndarray\n",
    "            The test data's labels.\n",
    "        \n",
    "        threshold : int or float \n",
    "            The threshold to determine the predicted label\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.eval_y = eval_y\n",
    "        if isinstance(eval_X, pd.DataFrame):\n",
    "            eval_X = eval_X.values\n",
    "            X_cols = X.columns\n",
    "        self.eval_X = eval_X\n",
    "        y_probs = model.predict_proba(eval_X)[:, 1]\n",
    "        self.y_probs = y_probs\n",
    "        if threshold == 0.5:\n",
    "            pred = model.predict(eval_X)\n",
    "        else:\n",
    "            pred = np.where(y_probs>threshold, 1, 0)       \n",
    "        accuracy = accuracy_score(pred, eval_y)\n",
    "        recall_1 = recall_score(eval_y, pred, pos_label=1)\n",
    "        precision_1 = precision_score(eval_y, pred, pos_label=1)\n",
    "        recall_0 = recall_score(eval_y, pred, pos_label=0)\n",
    "        precision_0 = precision_score(eval_y, pred, pos_label=0)\n",
    "        f1 = f1_score(eval_y, pred, pos_label=1)\n",
    "        roc_auc = roc_auc_score(eval_y, y_probs)\n",
    "        y_true = pd.Series(eval_y)\n",
    "        y_pred = pd.Series(pred)\n",
    "        confusion = pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "        return accuracy, recall_1, precision_1, recall_0, precision_0, f1, roc_auc, confusion\n",
    "        \n",
    "        \n",
    "    def evaluate(self, model, eval_X, eval_y, threshold=0.5, plot=True, save=False, save_folder=\"result\"):\n",
    "        \"\"\"Make prediction and generate evaluation report based on specified threshold.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : sklearn/lightgbm/xgboost/catboost classification model object\n",
    "            The model for evaluation.\n",
    "\n",
    "        eval_X : ndarray or pd.DataFrame\n",
    "            The test data's features.\n",
    "\n",
    "        eval_y : ndarray\n",
    "            The test data's labels.\n",
    "\n",
    "        plot : bool (default=True)\n",
    "            If \"Flase\", return only common metrics. If \"True\", return the plots including\n",
    "            ROC curve, Precision_Recall vs threshold, class probability distribution and\n",
    "            feature importance as well.\n",
    "\n",
    "        save : bool, optional (default=False)\n",
    "            Whether to save the result or not.\n",
    "\n",
    "        save_folder : string (default=\"result\")\n",
    "            The folder path to save the result, default is the result folder in the current directory.\n",
    "        \"\"\"\n",
    "        accuracy, recall_1, precision_1, recall_0, precision_0, f1, roc_auc, confusion = self.get_metrics(model, eval_X, eval_y, threshold)\n",
    "        print(\"Evaluation result of Threshold=={thres}\".format(thres=threshold))\n",
    "        print(\"---Common Metrics---\")\n",
    "        print(\"The accuracy is %0.4f\" % accuracy)\n",
    "        print(\"The recall for 1 is %0.4f\" % recall_1)\n",
    "        print(\"The precision for 1 is %0.4f\" % precision_1)\n",
    "        print(\"The recall for 0 is %0.4f\" % recall_0)\n",
    "        print(\"The precision for 0 is %0.4f\" % precision_0)\n",
    "        print(\"The F1-score is %0.4f\" % f1)\n",
    "        print(\"The ROC-AUC is %0.4f\" % roc_auc)\n",
    "        print(\"\\n---Confusion Matrix---\")\n",
    "        print(confusion)\n",
    "        if plot:\n",
    "            # AUC\n",
    "            fpr, tpr, auc_thresholds = roc_curve(eval_y, self.y_probs)\n",
    "            precisions, recalls, thresholds = precision_recall_curve(eval_y, self.y_probs)\n",
    "\n",
    "            # ROC\n",
    "            fig = plt.figure(figsize=(10, 27))\n",
    "            plt.subplots_adjust(hspace=0.25)\n",
    "            ax1 = fig.add_subplot(411)\n",
    "            ax1.set_title('ROC Curve')\n",
    "            ax1.plot(fpr, tpr, linewidth=2)\n",
    "            ax1.plot([0, 1], [0, 1], 'k--')\n",
    "            ax1.axis([-0.005, 1, 0, 1.005])\n",
    "            ax1.set_xticks(np.arange(0, 1, 0.05))\n",
    "            ax1.set_xlabel('False Positive Rate')\n",
    "            ax1.set_ylabel('True Positive Rate (Recall)')\n",
    "\n",
    "            # Recall_Precision VS Decision Threshold Plot\n",
    "            ax2 = fig.add_subplot(412)\n",
    "            ax2.set_title('Precision and Recall vs Decision Threshold')\n",
    "            ax2.plot(thresholds, precisions[:-1], 'b--', label='Precision')\n",
    "            ax2.plot(thresholds, recalls[:-1], 'g-', label='Recall')\n",
    "            ax2.set_ylabel('Score')\n",
    "            ax2.set_xlabel('Decision Threshold')\n",
    "            ax2.legend(loc='best')\n",
    "\n",
    "            # Class Probability Distribution\n",
    "            ax3 = fig.add_subplot(413)\n",
    "            ax3.set_title('Class Probability Distribution')\n",
    "            ax3.set_ylabel('Density')\n",
    "            ax3.set_xlabel('Predicted Probability')\n",
    "            ax3.hist(self.y_probs[eval_y == 1], bins=40,\n",
    "                           density=True, alpha=0.5)\n",
    "            ax3.hist(self.y_probs[eval_y == 0], bins=40,\n",
    "                              density=True, alpha=0.5)\n",
    "            \n",
    "            # Feature importance\n",
    "            model_list = [\"DecisionTree\",\"RandomForest\", \"XGB\", \"LGBM\"]\n",
    "            if any(mod_name in str(type(model)) for mod_name in model_list): \n",
    "                ax4 = fig.add_subplot(414)\n",
    "                ax4.set_title('Feature Importance')\n",
    "                feature_importance = model.feature_importances_\n",
    "                try:\n",
    "                    X_cols\n",
    "                except:\n",
    "                    pd.Series(feature_importance, index=range(eval_X.shape[1])).nlargest(eval_X.shape[1]).plot(kind='barh')\n",
    "                    ax4.set_ylabel('Column Index')\n",
    "                else:\n",
    "                    pd.Series(feature_importance, index=X_cols).nlargest(eval_X.shape[1]).plot(kind='barh', color = range(eval_X.shape[1]))\n",
    "        if save:\n",
    "            super(BinaryEvaluator, self).ensure_path(save_folder)\n",
    "            plot_path = save_folder + 'multiple_metrics_plots.png'\n",
    "            if fig:\n",
    "                fig.savefig(\n",
    "                    plot_path,\n",
    "                    bbox_inches='tight'\n",
    "                )\n",
    "            #Write result into a txt\n",
    "            output = '\\n'.join([\n",
    "                '--Model Evaluation--',\n",
    "                '\\tAccuracy: {accuracy:.4f}',\n",
    "                '\\tRecall for 1: {recall_1:.4f}',\n",
    "                '\\tPrecision for 1: {precision_1:.4f}',\n",
    "                '\\tRecall for 0: {recall_0:.4f}',\n",
    "                '\\tPrecision for 0: {precision_0:.4f}',\n",
    "                '\\tF1 score: {f1:.4f}',\n",
    "                '\\tROC-AUC: {roc_auc:.4f}',\n",
    "                '\\n',\n",
    "                '--Confusion Matrix--',\n",
    "                '{confusion}'\n",
    "            ]).format(\n",
    "                accuracy = accuracy,\n",
    "                recall_1 = recall_1,\n",
    "                precision_1 = precision_1,\n",
    "                recall_0 = recall_0,\n",
    "                precision_0 = precision_0,\n",
    "                f1 = f1,\n",
    "                roc_auc = roc_auc,\n",
    "                confusion = confusion\n",
    "            )\n",
    "            result_path = save_folder + 'clf_output.txt'\n",
    "            with open(result_path, 'w+') as f:\n",
    "                f.write(output)\n",
    "            \n",
    "\n",
    "    def ThresGridSearch(self, model, eval_X, eval_y, thres_list=None, objective=None):\n",
    "        \"\"\"Show the result of common metrics of given thresholds grid.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        thres_list : list (default=\"None\")\n",
    "            If no threshold list is given, it uses default grid:\n",
    "            [0.3, 0.4, 0.5, 0.6, 0.7], else, uses the given threshold\n",
    "            grid list.\n",
    " \n",
    "        objective : list (default=None)\n",
    "            Sort the result by given metrics result.\n",
    "            Possible choices are 'accuracy', 'recall_1',\n",
    "            precision_1', 'recall_0', 'precision_0', 'f1'.\n",
    "        \"\"\"\n",
    "        accuracy_list = []\n",
    "        recall_1_list = []\n",
    "        precision_1_list = []\n",
    "        recall_0_list = []\n",
    "        precision_0_list = []\n",
    "        f1_list = []\n",
    "        roc_auc_list = []\n",
    "        confusion_list = []\n",
    "        if thres_list == None:\n",
    "            thres_list = np.arange(0.3, 0.7, 0.1)\n",
    "        for thres in thres_list:\n",
    "            accuracy, recall_1, precision_1, recall_0, precision_0, f1, roc_auc, confusion = self.get_metrics(model, eval_X, eval_y, thres)\n",
    "            accuracy_list.append(accuracy)\n",
    "            recall_1_list.append(recall_1)\n",
    "            precision_1_list.append(precision_1)\n",
    "            recall_0_list.append(recall_0)\n",
    "            precision_0_list.append(precision_0)\n",
    "            f1_list.append(f1)\n",
    "            roc_auc_list.append(roc_auc)\n",
    "            confusion_list.append(confusion)\n",
    "        data = np.array([thres_list, accuracy_list, recall_1_list, precision_1_list, recall_0_list, precision_0_list, f1_list, roc_auc_list]).T\n",
    "        self.df = pd.DataFrame(data, columns=[\"Threshold\", \"accuracy\", \"recall_1\", \"precision_1\", \"recall_0\", \"precision_0\", \"f1\",\"roc_auc\"])\n",
    "        if objective:\n",
    "            self.df = self.df.sort_values(by=objective)\n",
    "        display(self.df)\n",
    "    \n",
    "    \n",
    "    def find_best_model(self, models, eval_X=None, eval_y=None, objective=\"accuracy\", threshold=0.5):\n",
    "        \"\"\"Find the best model with the specified objective metrics on given \n",
    "        threshold.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : sklearn/lightgbm/xgboost/catboost classification model object\n",
    "            The model for evaluation.\n",
    "\n",
    "        eval_X : ndarray or pd.DataFrame\n",
    "            The test data's features.\n",
    "\n",
    "        eval_y : ndarray\n",
    "            The test data's labels.\n",
    "            \n",
    "        objective: string (default=\"accuracy\")\n",
    "            The search goal     \n",
    "            \n",
    "        threshold: float (default=0.5)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        A single model object.\n",
    "        \"\"\"\n",
    "        result = np.array([])\n",
    "        for model in models:\n",
    "            accuracy, recall_1, precision_1, recall_0, precision_0, f1, _, _ = self.get_metrics(model, eval_X, eval_y, threshold)\n",
    "            if objective == \"accuracy\":\n",
    "                result = np.append(result, accuracy)\n",
    "            if objective == \"recall_1\":\n",
    "                result = np.append(result, recall_1)\n",
    "            if objective == \"precision_1\":\n",
    "                result = np.append(result, precision_1)\n",
    "            if objective == \"recall_0\":\n",
    "                result = np.append(result, recall_0)\n",
    "            if objective == \"precision_0\":\n",
    "                result = np.append(result, precision_0)\n",
    "            if objective == \"f1\":\n",
    "                result = np.append(result, f1)\n",
    "        print(\"The model with maximum %s (%s) is the %s th model\" % (objective, result[result.argmin()],result.argmin()+1))\n",
    "        return models[result.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
